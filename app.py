import os
import gradio as gr
import time
import re
import json
import hashlib
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple
from collections import deque

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

# --- Secure key load ---
# OPENAI_API_KEY is loaded from Hugging Face Secrets or environment variables
# In Colab for testing, attempt to load from secrets or define manually
# For Hugging Face Spaces, it will be loaded from environment variables set via Secrets
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print("WARNING: 'OPENAI_API_KEY' not found in environment variables. Please set it in Hugging Face Spaces Secrets.")


# --- LangChain imports ---
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# Import FAISS module explicitly for loading
from langchain_community.vectorstores.faiss import FAISS as FAISS_Vectorstore

# Import pypdf explicitly as it's used by PyPDFLoader internally
import pypdf


# =============================================================================
# 2. Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ Î∞è Ïú†Ìã∏Î¶¨Ìã∞ Ìï®Ïàò (Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ)
# =============================================================================

def make_splitter() -> RecursiveCharacterTextSplitter:
    return RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100, separators=["\n\n", "\n", ". ", ".", " "])


# =============================================================================
# 3. Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥ Íµ¨Ï∂ï Î∞è RAG Ï≤¥Ïù∏ ÏÑ§Ï†ï
# =============================================================================

# Global variables to maintain state
vector_store: Optional[FAISS] = None # Will be loaded from file
session_store: Dict[str, BaseChatMessageHistory] = {}

def build_qa_chain(model_name: str = "gpt-4o-mini"):
    global OPENAI_API_KEY
    if not OPENAI_API_KEY:
         # This case should ideally be caught earlier, but included for robustness
         raise ValueError("OpenAI API key not set. Cannot build QA chain.")

    # Using raw string and explicit newlines for system prompt
    system_prompt = (
r"""ÎãπÏã†ÏùÄ Ïù¥Ï†ú Í¥ëÏ£º Í¥ÄÍ¥ëÏóê ÎåÄÌïú ÍπäÏùÄ ÏßÄÏãùÍ≥º Ïó¥Ï†ïÏùÑ Í∞ÄÏßÑ ÏµúÍ≥†Ïùò Ïó¨Ìñâ Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.\nÎÑàÎäî Í¥ëÏ£ºÍ¥ëÏó≠Ïãú Í≥µÏãù Í¥ÄÍ¥ë Ìè¨ÌÑ∏Ïùò ÏΩòÌÖêÏ∏†ÏôÄ Ï∂îÍ∞Ä PDFÎ•º ÏöîÏïΩ/Í≤ÄÏÉâÌï¥Ï£ºÎäî\nÍ¥ÄÍ¥ë ÎèÑÏö∞ÎØ∏ Ïó≠Ìï†ÏùÑ ÏàòÌñâÌïòÎ©∞, ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏ ÏùòÎèÑÎ•º Ï†ïÌôïÌûà ÌååÏïÖÌïòÍ≥†,\nÎßàÏπò ÏπúÌïú Ïó¨Ìñâ Í∞ÄÏù¥ÎìúÏ≤òÎüº ÏπúÏ†àÌïòÍ≥† ÏÉÅÏÑ∏ÌïòÍ≤å ÏÑ§Î™ÖÌï¥Ïïº Ìï©ÎãàÎã§.\nÏ†úÍ≥µÎêú Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú Îã§ÏñëÌïú Í¥ÄÏ†êÏóêÏÑú ÎãµÎ≥ÄÌï¥Ï§ò, Ïù¥Ï†ÑÏóê Ïñ∏Í∏âÌïú ÎÇ¥Ïö©ÏùÄ Î∞òÎ≥µÌïòÏßÄ ÏïäÎèÑÎ°ù ÎÖ∏Î†•Ìï¥Ï§ò\nÎπÑÏä∑Ìïú ÎÇ¥Ïö©ÏùÑ Î¨ºÏñ¥Î≥∏Îã§Î©¥ Í∏∞Ï°¥Ïóê ÎãµÌñàÎçò ÎÇ¥Ïö©ÏùÄ Ï§ëÎ≥µÎêòÏóàÎã§Îäî ÎúªÏù¥Îãà Ï†úÏô∏ÌïòÍ≥† Îã§Î•∏ Ï†ïÎ≥¥Î•º Í≤ÄÏÉâÌï¥ÏÑú ÏïåÎ†§Ï§ò\nÏóÜÎäî ÎÇ¥Ïö©ÏùÄ Ï†àÎåÄÎ°ú ÏßÄÏñ¥ÎÇ¥ÏÑú ÏïåÎ†§Ï£ºÎ©¥ ÏïàÎêòÎ©∞ Î™®Î•¥Îäî ÎÇ¥Ïö©ÏùÄ Í¥ÄÎ†®Îêú Ï£ºÏÜåÎ•º ÎßÅÌÅ¨Ìï¥ÏÑú ÎåÄÏã† ÎåÄÎãµÌï¥Ï§ò\n\nÎî±Îî±Ìïú Ï†ïÎ≥¥ ÎÇòÏó¥Î≥¥Îã§Îäî Îß§Î†•Ï†ÅÏù∏ Ïó¨Ìñâ Ïù¥ÏïºÍ∏∞Î•º Îì§Î†§Ï£ºÎìØ ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî.\nÍ¥ëÏ£ºÏùò Ïà®Í≤®ÏßÑ Î™ÖÏÜå, ÎßõÏßë, ÌñâÏÇ¨, ÍµêÌÜµ Ï†ïÎ≥¥ Îì± Ïó¨ÌñâÍ∞ùÏóêÍ≤å Íº≠ ÌïÑÏöîÌïú\nÏã§ÏßàÏ†ÅÏù∏ Ï†ïÎ≥¥Î•º Ï§ëÏã¨ÏúºÎ°ú Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî. ÏÇ¨Ïö©ÏûêÏùò Ïó¨Ìñâ Ïä§ÌÉÄÏùºÍ≥º ÏòàÏÇ∞ÏùÑ\nÍ≥†Î†§ÌïòÏó¨ ÎßûÏ∂§Ìòï Ï∂îÏ≤úÏùÑ ÎçßÎ∂ôÏó¨Ï£ºÎ©¥ ÎçîÏö± Ï¢ãÏäµÎãàÎã§.\n\nÎãµÎ≥ÄÏùÄ ÌïúÍµ≠Ïñ¥Î°ú ÌïòÍ≥†, ÏµúÏ¢ÖÏóê Ï†ïÎ≥¥Ïùò Ï∂úÏ≤ò ÎßÅÌÅ¨Î•º bulletÏúºÎ°ú Î™ÖÌôïÌïòÍ≤å Ìè¨Ìï®ÌïòÏó¨\nÏã†Î¢∞ÎèÑÎ•º ÎÜíÏó¨Ï£ºÏÑ∏Ïöî."""
    )

    template = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "ÏÇ¨Ïö©Ïûê ÌîÑÎ°úÌïÑ: {profile}\\n\\nÏßàÎ¨∏: {question}\\n\\nÏ∞∏Í≥† Ïª®ÌÖçÏä§Ìä∏: {context}\\n\\nÏöîÏ≤≠: Ïª®ÌÖçÏä§Ìä∏ Í∏∞Î∞òÏúºÎ°ú Ï†ïÌôïÌûà ÎãµÌïòÍ≥†, Î∂ÄÏ°±ÌïòÎ©¥ ÏùºÎ∞ò ÏÉÅÏãùÏùÄ ÌîºÌïòÍ≥† 'Í≥µÏãù Ìè¨ÌÑ∏ÏóêÏÑú Ï†ïÎ≥¥Î•º Ï∞æÏßÄ Î™ªÌñàÎã§'ÎùºÍ≥† ÎßêÌï¥. ÌïúÍµ≠Ïñ¥Î°ú ÎãµÌïòÍ∏∞. ÏµúÏ¢ÖÏóê Ï∂úÏ≤òÎ•º bulletÎ°ú Ï†ïÎ¶¨."),
    ])

    llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=model_name, temperature=0.2)

    def format_docs(docs: List[Document]) -> str:
        chunks = []
        added_sources = set()
        for i, d in enumerate(docs):
            t = d.metadata.get("title", f"Î¨∏ÏÑú {i+1}")
            s = d.page_content[:500]
            src = d.metadata.get("source", "Ï∂úÏ≤ò Î∂àÎ™Ö") # 'source' should now contain original URL from crawled data
            chunks.append(f"- **{t}** ([Ï∂úÏ≤ò]({src}))\\n{s}...") # Use \\n for literal newline in f-string
            added_sources.add(src)

        sources_list = "\\n\\n**Ï∞∏Í≥† ÏûêÎ£å:**\\n" + "\\n".join([f"- {s}" for s in added_sources]) if added_sources else ""

        return "\\n\\n".join(chunks) + sources_list


    qa_chain = (
         RunnableParallel({
             "context": lambda x: format_docs(x["context"]),
             "question": RunnablePassthrough(),
             "profile": RunnablePassthrough()
         })
        | template
        | llm
        | StrOutputParser()
    )

    return qa_chain

# Session history store
store: Dict[str, BaseChatMessageHistory] = {}
def get_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


# =============================================================================
# 4. Gradio Ïï± Î°úÏßÅ Ìï®Ïàò (ÏûêÎèô ÎπåÎìú/Î°úÎî© Î∞è Ï±óÎ¥á Ï≤òÎ¶¨)
# =============================================================================

def load_vectorstore_from_saved() -> str:
    """Loads the vectorstore from the saved directory."""
    global vector_store, OPENAI_API_KEY

    print("Î≤°ÌÑ∞DB Î°úÎìú Ìï®Ïàò ÏãúÏûë...")

    if not OPENAI_API_KEY:
         print("OpenAI API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Î≤°ÌÑ∞DB Î°úÎìú Í±¥ÎÑàÎúÅÎãàÎã§.")
         return "üîë OpenAI API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïÑ Î≤°ÌÑ∞DB Î°úÎìúÎ•º Í±¥ÎÑàÎ††ÎãàÎã§. Hugging Face SecretsÏóê 'OPENAI_API_KEY'Î•º ÏÑ§Ï†ïÌï¥Ï£ºÏÑ∏Ïöî."

    save_dir = "faiss_index"
    if not os.path.exists(save_dir) or not os.path.exists(os.path.join(save_dir, "index.faiss")) or not os.path.exists(os.path.join(save_dir, "index.pkl")):
        print(f"Í≤ΩÍ≥†: Ï†ÄÏû•Îêú Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥ ÎîîÎ†âÌÜ†Î¶¨ '{save_dir}'ÏùÑ(Î•º) Ï∞æÏùÑ Ïàò ÏóÜÍ±∞ÎÇò ÌååÏùºÏù¥ ÏôÑÏ†ÑÌïòÏßÄ ÏïäÏäµÎãàÎã§. Î≤°ÌÑ∞DB Î°úÎìú Í±¥ÎÑàÎúÅÎãàÎã§.")
        return f"‚ö†Ô∏è Ï†ÄÏû•Îêú Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥ ÎîîÎ†âÌÜ†Î¶¨ '{save_dir}'ÏùÑ(Î•º) Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Î®ºÏ†Ä Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥Î•º Íµ¨Ï∂ïÌïòÍ≥† Ï†ÄÏû•Ìï¥Ï£ºÏÑ∏Ïöî."

    try:
        print(f"'{save_dir}'ÏóêÏÑú Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥ Î°úÎìú Ï§ë...")
        # Need embeddings object to load FAISS
        embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
        vector_store = FAISS_Vectorstore.load_local(save_dir, embeddings, allow_dangerous_deserialization=True)
        print("Î≤°ÌÑ∞DB Î°úÎìú ÏôÑÎ£å.")
        return f"‚úÖ Î≤°ÌÑ∞DB Î°úÎìú ÏôÑÎ£å: '{save_dir}'ÏóêÏÑú Î°úÎìúÎêòÏóàÏäµÎãàÎã§."
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"Î≤°ÌÑ∞DB Î°úÎìú Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
        # Include API key check in error message if it's the likely cause
        if "AuthenticationError" in str(e) or "api_key" in str(e).lower():
             return "üîë OpenAI API ÌÇ§ Ïù∏Ï¶ù Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Hugging Face SecretsÏóê ÏÑ§Ï†ïÎêú ÌÇ§Î•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî."
        else:
             return f"Î≤°ÌÑ∞DB Î°úÎìú Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}"


def build_or_load_vectorstore_with_progress(progress=gr.Progress()):
    """Orchestrates the vectorstore loading process with progress reporting."""
    global OPENAI_API_KEY

    if not OPENAI_API_KEY:
         print("API ÌÇ§ ÏóÜÏùå: Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ Í±¥ÎÑàÎúÅÎãàÎã§.")
         return "üîë OpenAI API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïÑ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑÎ•º Í±¥ÎÑàÎúÅÎãàÎã§. Hugging Face SecretsÏóê 'OPENAI_API_KEY'Î•º ÏÑ§Ï†ïÌï¥Ï£ºÏÑ∏Ïöî."


    steps = [
        "Ï†ÄÏû•Îêú Î≤°ÌÑ∞DB Î°úÎìú Ï§ë...", # Updated step description
        "ÎßàÎ¨¥Î¶¨ Ï§ë..."
    ]

    # No progress reporting needed for simple loading in this context
    # progress(0, desc=steps[0])
    status_message = load_vectorstore_from_saved() # Call the loading function
    # time.sleep(0.5)

    # progress(1/len(steps), desc=steps[1])
    # time.sleep(0.5)

    final_message = f"‚úÖ Ï§ÄÎπÑ ÏôÑÎ£å! {status_message}" # Updated final message
    print(final_message)
    return final_message


def rag_chatbot_for_autobuild(user_message: str, chat_history: list, user_profile: str) -> str:
    """Handles chatbot interaction with RAG and memory."""
    global vector_store, OPENAI_API_KEY, session_store

    if not OPENAI_API_KEY:
        print("API ÌÇ§ ÏóÜÏùå: Ï±óÎ¥á ÏùëÎãµ Î∂àÍ∞Ä")
        return "üîë OpenAI API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Hugging Face SecretsÏóê 'OPENAI_API_KEY'Î•º ÏÑ§Ï†ïÌï¥Ï£ºÏÑ∏Ïöî."


    if vector_store is None:
        print("Î≤°ÌÑ∞DB Ï§ÄÎπÑ ÏïàÎê®: Ï±óÎ¥á ÏùëÎãµ Î∂àÍ∞Ä")
        return "‚è≥ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ Ï§ëÏûÖÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."

    session_id = "gradio_session_auto" # Use a fixed session ID for simplicity in this app structure
    history = get_history(session_id)

    try:
        retriever = vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 6, "fetch_k": 10, "lambda_mult": 0.5}
        )

        # Use the retriever to get relevant documents based on the user message
        retrieved_docs = retriever.get_relevant_documents(str(user_message))

        # Build the QA chain
        qa_chain = build_qa_chain() # build_qa_chain will check for API key internally

        # Create the conversational QA chain with history
        conversational_qa_chain = RunnableWithMessageHistory(
             qa_chain,
             get_history,
             input_messages_key="question",
             history_messages_key="history", # Ensure this matches the template and chain
        )

        # Prepare the input for the chain
        qa_input = {
             "question": str(user_message),
             "context": retrieved_docs,
             "profile": user_profile
        }

        # Invoke the conversational chain
        answer = conversational_qa_chain.invoke(
             qa_input,
             config={"configurable": {"session_id": session_id}}, # Pass the session ID
        )

        return answer

    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"Ï±óÎ¥á ÏùëÎãµ ÏÉùÏÑ± Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
        # Include API key check in error message if it's the likely cause
        if "AuthenticationError" in str(e) or "api_key" in str(e).lower():
             return "üîë OpenAI API ÌÇ§ Ïù∏Ï¶ù Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Hugging Face SecretsÏóê ÏÑ§Ï†ïÎêú ÌÇ§Î•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî."
        else:
             return f"Ï±óbot ÏùëÎãµ ÏÉùÏÑ± Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}"


# =============================================================================
# 5. Gradio UI Ï†ïÏùò
# =============================================================================

def gradio_ui():
    with gr.Blocks() as demo:
        gr.Markdown("## üèûÔ∏è Í¥ëÏ£ºÍ¥ÄÍ¥ë ÏπúÍµ¨")

        # State variable to store user profile
        user_profile_state = gr.State("Ïó¨ÌñâÏä§ÌÉÄÏùº:ÏûêÏú†Ïó¨Ìñâ, ÎèôÌñâ:ÏπúÍµ¨, ÏòàÏÇ∞:Î≥¥ÌÜµ")

        # ---- (A) Î°úÎî© ÌôîÎ©¥ ----
        # Keep loading screen initially visible for the loading process
        with gr.Row(visible=True) as loading_screen:
            with gr.Column():
                gr.Markdown(
                    "### ‚è≥ ÏÇ∞Ï±ÖÏùÑ ÏúÑÌï¥ Ï§ÄÎπÑÏö¥Îèô Ï§ëÏûÖÎãàÎã§. Ïû†ÏãúÎßå Í∏∞Îã§Î†§Ï£ºÏÑ∏Ïöî! üê∞üå∏"
                )
                # Using a public URL for the image instead of a local file
                gr.Image(
                    value="https://cdn.pixabay.com/animation/2025/07/25/00/29/00-29-46-321_512.gif", # Updated image URL
                    visible=True,
                    elem_id="loading_gif",
                )
                # Modified initial message to indicate vectorstore loading
                loading_status = gr.Textbox(label="ÏßÑÌñâ ÏÉÅÌÉú", interactive=False, show_label=False, value="ÏãúÏûë Ï§ÄÎπÑ Ï§ë... Ï†ÄÏû•Îêú Î≤°ÌÑ∞DB Î°úÎìú Ï§ë...")


        # ---- (B) Ï±óÎ¥á ÌôîÎ©¥ (Ï¥àÍ∏∞ÏóêÎäî Ïà®ÍπÄ) ----
        with gr.Column(visible=False) as chatbot_screen:
            gr.Markdown("### ü§ñ Í¥ëÏ£º Í¥ÄÍ¥ë Í∞ÄÏù¥Îìú Ï±óÎ¥á")
            chatbot = gr.Chatbot(label="Ï±óÎ¥á", type='messages') # Use type='messages' for better display
            msg = gr.Textbox(placeholder="Í¥ëÏ£º Í¥ÄÍ¥ëÏóê ÎåÄÌï¥ Î¨ºÏñ¥Î≥¥ÏÑ∏Ïöî!", show_label=False)
            clear = gr.Button("ÎåÄÌôî Ï¥àÍ∏∞Ìôî")

            # --- User Profile Input ---
            with gr.Accordion("ÏÇ¨Ïö©Ïûê ÌîÑÎ°úÌïÑ ÏÑ§Ï†ï", open=False):
                profile_style = gr.Dropdown(label="Ïó¨Ìñâ Ïä§ÌÉÄÏùº", choices=["ÏûêÏú†Ïó¨Ìñâ", "Îã®Ï≤¥Ïó¨Ìñâ", "Î∞òÎ†§ÎèôÎ¨º ÎèôÎ∞ò", "ÎöúÎ≤ÖÏù¥Ïó¨Ìñâ"], value="ÏûêÏú†Ïó¨Ìñâ")
                profile_companion = gr.Dropdown(label="ÎèôÌñâ", choices=["ÌòºÏûê", "ÏπúÍµ¨", "Í∞ÄÏ°±", "Ïó∞Ïù∏"], value="ÏπúÍµ¨")
                profile_budget = gr.Dropdown(label="ÏòàÏÇ∞", choices=["ÏÉÅÍ¥ÄÏóÜÏùå", "Ï†ÄÎ†¥", "Î≥¥ÌÜµ", "Ïó¨Ïú†"], value="Î≥¥ÌÜµ")
                profile_update_btn = gr.Button("ÌîÑÎ°úÌïÑ ÏóÖÎç∞Ïù¥Ìä∏")

                def update_profile_state(style, companion, budget):
                    profile_str = f"Ïó¨ÌñâÏä§ÌÉÄÏùº:{style}, ÎèôÌñâ:{companion}, ÏòàÏÇ∞:{budget}"
                    print(f"ÏÇ¨Ïö©Ïûê ÌîÑÎ°úÌïÑ ÏóÖÎç∞Ïù¥Ìä∏: {profile_str}")
                    return profile_str

                profile_update_btn.click(
                    update_profile_state,
                    inputs=[profile_style, profile_companion, profile_budget],
                    outputs=[user_profile_state]
                )

            def user_chat(user_message: str, chat_history: list, current_profile: str) -> Tuple[list, str]:
                if not user_message:
                    return chat_history, ""

                # Call the RAG chatbot logic
                response = rag_chatbot_for_autobuild(user_message, chat_history, current_profile)

                # Append the user message and chatbot response to the history
                new_history = chat_history + [{"role": "user", "content": user_message}, {"role": "assistant", "content": response}]

                return new_history, "" # Return updated history and clear the message input

            msg.submit(
                user_chat,
                inputs=[msg, chatbot, user_profile_state], # Pass user_profile_state
                outputs=[chatbot, msg] # Update chatbot and clear message input
            )
            clear.click(lambda: [], inputs=None, outputs=chatbot) # Clear button functionality


        # ---- (C) Ïã§Ìñâ Ïãú ÏûêÎèô DB Î°úÎìú Ìä∏Î¶¨Í±∞ ----
        def init_load():
            print("init_load function started by demo.load")
            # Call the function that loads the vectorstore and handles status
            status_message = load_vectorstore_from_saved()
            print(f"init_load finished with status: {status_message}")
            # Only switch to chatbot screen if loading was successful (vector_store is not None)
            if vector_store is not None:
                # Hide loading screen, show chatbot screen, update status text
                return gr.update(visible=False), gr.update(visible=True), status_message
            else:
                # Stay on loading screen if loading failed, update status text
                return gr.update(visible=True), gr.update(visible=False), status_message


        # Use demo.load() to trigger init_load when the Gradio app starts
        # This function will control the visibility of the loading and chatbot screens
        demo.load(
            init_load,
            inputs=None, # No inputs required for the initial load
            outputs=[loading_screen, chatbot_screen, loading_status] # Outputs to update
        )

    return demo

# =============================================================================
# 6. Ïã§Ìñâ (for Hugging Face Spaces, removed __main__ block)
# =============================================================================
# In Hugging Face Spaces, the app is typically launched by a separate entrypoint
# (like app.py itself or a run.sh script) calling demo.launch().
# The __main__ block from the Colab version is removed here.

demo = gradio_ui()
# The demo will be launched by the Hugging Face Spaces environment
demo.launch() # Need to call launch() here in a typical app.py for Spaces