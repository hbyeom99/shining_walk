
import os
import gradio as gr
import time
import re
import json
import hashlib
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple
from collections import deque

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

# --- Secure key load ---
# OPENAI_API_KEY is loaded from Hugging Face Secrets or environment variables
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# --- LangChain imports ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory


# =============================================================================
# 2. ë°ì´í„° êµ¬ì¡° ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (í¬ë¡¤ë§, ë°ì´í„° ì¤€ë¹„)
# =============================================================================
BASE_URL = "https://tour.gwangju.go.kr"
HOME_URL = f"{BASE_URL}/home/main.cs"
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/127.0.0.1 Safari/537.36"
)

@dataclass
class CrawlItem:
    title: str
    link: str
    summary: str
    lang: str

def get_soup(url: str) -> Optional[BeautifulSoup]:
    try:
        r = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=15)
        if r.status_code != 200:
            print(f"Error fetching {url}: Status Code {r.status_code}")
            return None
        return BeautifulSoup(r.text, "html.parser")
    except Exception as e:
        print(f"Exception fetching {url}: {e}")
        return None

def absolutize(href: str) -> str:
    if href.startswith("http"):
        return href
    if href.startswith("/"):
        return BASE_URL + href
    return BASE_URL + "/" + href.lstrip("/")

SECTION_HREF_PATTERNS = ["/sub.cs?m=", "/eng/", "/tour/info/"]
# Using raw string for regex to avoid issues with backslashes
DETAIL_PATTERNS = [re.compile(r"/tour/info/[^?]+\.cs\?act=view&.*infoId=\d+")]

def extract_text(soup: BeautifulSoup) -> str:
    for tag in soup(["script", "style", "noscript", "header", "footer", "nav", "form", "aside"]):
        tag.extract()
    summary_candidates = []
    for cls in ["summary", "desc", "text", "txt", "cont", "con_txt", "info", "basic", "bbs_view", "view_cont", "body_wrap"]:
        for el in soup.select(f".{cls}"):
            txt = el.get_text(" ", strip=True)
            if txt and len(txt) > 50:
                summary_candidates.append(txt)
    for p in soup.select("div.contents p, div.body p, main p"):
        txt = p.get_text(" ", strip=True)
        if txt and len(txt) > 50:
            summary_candidates.append(txt)

    if summary_candidates:
        best = max(summary_candidates, key=len)
    else:
        body_text = soup.body.get_text(" ", strip=True) if soup.body else ""
        best = body_text

    # Using raw string for regex
    return re.sub(r"\s+", " ", best)[:1500]

def detect_lang(text: str) -> str:
    return "ko" if re.search(r"[\u3131-\uD7A3]", text) else "en"

def crawl_home_and_details(start_url: str, max_pages: int = 140, delay_sec: float = 0.5) -> List[CrawlItem]:
    items: List[CrawlItem] = []
    to_crawl_queue = deque([start_url])
    processed_urls: set[str] = set([start_url])

    page_count = 0

    print(f"í¬ë¡¤ë§ ì‹œì‘: {start_url}")

    pbar = tqdm(total=max_pages, desc="í¬ë¡¤ë§ ì§„í–‰", unit="í˜ì´ì§€")

    while to_crawl_queue and page_count < max_pages:
        current_url = to_crawl_queue.popleft()
        pbar.set_description(f"í¬ë¡¤ë§ ì¤‘: {current_url[:60]}...")

        soup = get_soup(current_url)
        if not soup:
            pbar.write(f"  > í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {current_url}")
            continue

        page_count += 1
        pbar.update(1)

        if any(p.search(current_url) for p in DETAIL_PATTERNS):
            title = None
            for sel in ["h1", "h2", ".tit", ".title", ".page_tit", ".bbs_tit", "meta[property='og:title']"]:
                el = soup.select_one(sel)
                if el:
                    title = el.get("content") if el.name == "meta" else el.get_text(strip=True)
                    break
            if not title:
                title = soup.title.get_text(strip=True) if soup.title else current_url
            summary = extract_text(soup)
            lang = detect_lang(summary)
            if len(summary) > 50:
                 items.append(CrawlItem(title=title, link=current_url, summary=summary, lang=lang))
                 pbar.write(f"  > ìƒì„¸ í˜ì´ì§€ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {title[:50]}...")


        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            absolute_url = absolutize(href)

            if not absolute_url.startswith(BASE_URL): continue
            if "#" in absolute_url: absolute_url = absolute_url.split("#")[0]
            if absolute_url.endswith((".zip", ".pdf", ".doc", ".xls", ".ppt", ".hwp", ".png", ".jpg", ".jpeg", ".gif", ".mp4", ".avi", ".wmv")): continue
            if "javascript:" in absolute_url: continue
            if "mailto:" in absolute_url: continue

            is_section_link = any(p in absolute_url for p in SECTION_HREF_PATTERNS)
            is_detail_link = any(p.search(absolute_url) for p in DETAIL_PATTERNS)
            is_pagination_link = "page=" in absolute_url

            if (is_section_link or is_detail_link or is_pagination_link) and absolute_url not in processed_urls:
                 to_crawl_queue.append(absolute_url)
                 processed_urls.add(absolute_url)


        time.sleep(delay_sec)

    pbar.close()
    print(f"í¬ë¡¤ë§ ì™„ë£Œ. ì´ {len(items)}ê±´ì˜ ë°ì´í„° ì¶”ì¶œ.")
    print(f"ì²˜ë¦¬ëœ URL ìˆ˜: {len(processed_urls)}")
    return items


def items_to_documents(items: List[CrawlItem]) -> List[Document]:
    docs = []
    for it in items:
        meta = {"source": it.link, "title": it.title, "lang": it.lang}
        content = it.summary if isinstance(it.summary, str) else str(it.summary)
        docs.append(Document(page_content=content, metadata=meta))
    return docs

def make_splitter() -> RecursiveCharacterTextSplitter:
    return RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100, separators=["\n\n", "\n", ". ", ".", " "])

def load_pdf_urls(pdf_urls: List[str]) -> List[Document]:
    loaded: List[Document] = []
    cache_dir = ".cache_pdfs"
    os.makedirs(cache_dir, exist_ok=True)
    for url in tqdm(pdf_urls, desc="PDF ë‹¤ìš´ë¡œë“œ ë° ë¡œë”©"):
        try:
            fn = os.path.join(cache_dir, hashlib.md5(url.encode()).hexdigest() + ".pdf")
            if not os.path.exists(fn):
                r = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=60, stream=True)
                r.raise_for_status()
                total_size = int(r.headers.get('content-length', 0))
                with open(fn, "wb") as f, tqdm(
                    total=total_size, unit='B', unit_scale=True, desc=url.split('/')[-1], leave=False
                ) as pbar:
                    for chunk in r.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            pbar.update(len(chunk))
            if os.path.exists(fn):
                loader = PyPDFLoader(fn)
                try:
                    pdf_docs = loader.load()
                    loaded.extend(pdf_docs)
                except Exception as pdf_e:
                    print(f"Error loading PDF {url}: {pdf_e}")
        except Exception as e:
            print(f"Error downloading or processing PDF {url}: {e}")
            continue
    return loaded

# =============================================================================
# 3. ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ë° RAG ì²´ì¸ ì„¤ì •
# =============================================================================

# Global variables to maintain state
crawled_items: List[CrawlItem] = []
vector_store: Optional[FAISS] = None
session_store: Dict[str, BaseChatMessageHistory] = {}

def build_faiss(docs: List[Document]) -> FAISS:
    if not OPENAI_API_KEY:
        # This check should ideally happen before calling build_faiss
        # but is included here as a safeguard.
        raise ValueError("OpenAI API key not set. Cannot build embeddings.")

    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
    return FAISS.from_documents(docs, embeddings)

def build_qa_chain(model_name: str = "gpt-4o-mini"):
    if not OPENAI_API_KEY:
         # This check should ideally happen before calling build_qa_chain
         # but is included here as a safeguard.
         raise ValueError("OpenAI API key not set. Cannot build QA chain.")

    # Using raw string and explicit newlines for system prompt
    system_prompt = (
r"""ë‹¹ì‹ ì€ ì´ì œ ê´‘ì£¼ ê´€ê´‘ì— ëŒ€í•œ ê¹Šì€ ì§€ì‹ê³¼ ì—´ì •ì„ ê°€ì§„ ìµœê³ ì˜ ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\në„ˆëŠ” ê´‘ì£¼ê´‘ì—­ì‹œ ê³µì‹ ê´€ê´‘ í¬í„¸ì˜ ì½˜í…ì¸ ì™€ ì¶”ê°€ PDFë¥¼ ìš”ì•½/ê²€ìƒ‰í•´ì£¼ëŠ”\nê´€ê´‘ ë„ìš°ë¯¸ ì—­í• ì„ ìˆ˜í–‰í•˜ë©°, ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³ ,\në§ˆì¹˜ ì¹œí•œ ì—¬í–‰ ê°€ì´ë“œì²˜ëŸ¼ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.\nì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ë‹µë³€í•´ì¤˜, ì´ì „ì— ì–¸ê¸‰í•œ ë‚´ìš©ì€ ë°˜ë³µí•˜ì§€ ì•Šë„ë¡ ë…¸ë ¥í•´ì¤˜\në¹„ìŠ·í•œ ë‚´ìš©ì„ ë¬¼ì–´ë³¸ë‹¤ë©´ ê¸°ì¡´ì— ë‹µí–ˆë˜ ë‚´ìš©ì€ ì¤‘ë³µë˜ì—ˆë‹¤ëŠ” ëœ»ì´ë‹ˆ ì œì™¸í•˜ê³  ë‹¤ë¥¸ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ì„œ ì•Œë ¤ì¤˜\nì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ë¡œ ì§€ì–´ë‚´ì„œ ì•Œë ¤ì£¼ë©´ ì•ˆë˜ë©° ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ê´€ë ¨ëœ ì£¼ì†Œë¥¼ ë§í¬í•´ì„œ ëŒ€ì‹  ëŒ€ë‹µí•´ì¤˜\n\në”±ë”±í•œ ì •ë³´ ë‚˜ì—´ë³´ë‹¤ëŠ” ë§¤ë ¥ì ì¸ ì—¬í–‰ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ë“¯ ë‹µë³€í•´ì£¼ì„¸ìš”.\nê´‘ì£¼ì˜ ìˆ¨ê²¨ì§„ ëª…ì†Œ, ë§›ì§‘, í–‰ì‚¬, êµí†µ ì •ë³´ ë“± ì—¬í–‰ê°ì—ê²Œ ê¼­ í•„ìš”í•œ\nì‹¤ì§ˆì ì¸ ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. ì‚¬ìš©ìì˜ ì—¬í–‰ ìŠ¤íƒ€ì¼ê³¼ ì˜ˆì‚°ì„\nê³ ë ¤í•˜ì—¬ ë§ì¶¤í˜• ì¶”ì²œì„ ë§ë¶™ì—¬ì£¼ë©´ ë”ìš± ì¢‹ìŠµë‹ˆë‹¤.\n\në‹µë³€ì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ìµœì¢…ì— ì •ë³´ì˜ ì¶œì²˜ ë§í¬ë¥¼ bulletìœ¼ë¡œ ëª…í™•í•˜ê²Œ í¬í•¨í•˜ì—¬\nì‹ ë¢°ë„ë¥¼ ë†’ì—¬ì£¼ì„¸ìš”."""
    )

    template = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "ì‚¬ìš©ì í”„ë¡œí•„: {profile}\\n\\nì§ˆë¬¸: {question}\\n\\nì°¸ê³  ì»¨í…ìŠ¤íŠ¸: {context}\\n\\nìš”ì²­: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ë‹µí•˜ê³ , ë¶€ì¡±í•˜ë©´ ì¼ë°˜ ìƒì‹ì€ í”¼í•˜ê³  'ê³µì‹ í¬í„¸ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤'ë¼ê³  ë§í•´. í•œêµ­ì–´ë¡œ ë‹µí•˜ê¸°. ìµœì¢…ì— ì¶œì²˜ë¥¼ bulletë¡œ ì •ë¦¬."),
    ])

    llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=model_name, temperature=0.2)

    def format_docs(docs: List[Document]) -> str:
        chunks = []
        added_sources = set()
        for i, d in enumerate(docs):
            t = d.metadata.get("title", f"ë¬¸ì„œ {i+1}")
            s = d.page_content[:500]
            src = d.metadata.get("source", "ì¶œì²˜ ë¶ˆëª…")
            chunks.append(f"- **{t}** ([ì¶œì²˜]({src}))\\n{s}...") # Use \\n for literal newline in f-string
            added_sources.add(src)

        sources_list = "\\n\\n**ì°¸ê³  ìë£Œ:**\\n" + "\\n".join([f"- {s}" for s in added_sources]) if added_sources else ""

        return "\\n\\n".join(chunks) + sources_list


    qa_chain = (
         RunnableParallel({
             "context": lambda x: format_docs(x["context"]),
             "question": RunnablePassthrough(),
             "profile": RunnablePassthrough()
         })
        | template
        | llm
        | StrOutputParser()
    )

    return qa_chain

store: Dict[str, BaseChatMessageHistory] = {}
def get_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


# =============================================================================
# 4. Gradio ì•± ë¡œì§ í•¨ìˆ˜ (ìë™ ë¹Œë“œ ë° ì±—ë´‡ ì²˜ë¦¬)
# =============================================================================

def crawl_data_for_autobuild() -> str:
    """Performs crawling for the autobuild process."""
    global crawled_items
    print("í¬ë¡¤ë§ í•¨ìˆ˜ ì‹œì‘...")
    try:
        crawled_items = crawl_home_and_details(start_url=HOME_URL, max_pages=140, delay_sec=0.2)
        uniq: Dict[str, CrawlItem] = {}
        for it in crawled_items:
            uniq[it.link] = it
        crawled_items = list(uniq.values())

        if crawled_items:
            print(f"í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(crawled_items)}ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return f"í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(crawled_items)}ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤."
        else:
            print("í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return "í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def build_vectorstore_for_autobuild() -> str:
    """Builds vectorstore for the autobuild process."""
    global crawled_items, vector_store, OPENAI_API_KEY

    print("ë²¡í„°DB êµ¬ì¶• í•¨ìˆ˜ ì‹œì‘...")

    if not OPENAI_API_KEY:
         print("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë²¡í„°DB êµ¬ì¶• ê±´ë„ˆëœë‹ˆë‹¤.")
         # Modified message to guide user on Hugging Face Secrets
         return "ğŸ”‘ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ë²¡í„°DB êµ¬ì¶•ì„ ê±´ë„ˆëœë‹ˆë‹¤. Hugging Face Secretsì— 'OPENAI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."


    if not crawled_items:
        print("í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë²¡í„°DB êµ¬ì¶• ê±´ë„ˆëœë‹ˆë‹¤.")
        return "í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë²¡í„°DB êµ¬ì¶• ê±´ë„ˆëœë‹ˆë‹¤."

    docs = items_to_documents(crawled_items)

    # Example PDF URLs (add actual URLs if needed)
    # pdf_urls = ["http://example.com/some_gwangju_guide.pdf"]
    # if pdf_urls:
    #      print(f"ì¶”ê°€ PDF {len(pdf_urls)}ê±´ ë¡œë”© ì¤‘...")
    #      pdf_docs = load_pdf_urls(pdf_urls)
    #      docs.extend(pdf_docs)
    #      print(f"PDF ë¡œë”© ì™„ë£Œ. ì´ {len(pdf_docs)}ê±´ ì¶”ê°€.")


    if not docs:
        print("ë¬¸ì„œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë²¡í„°DB êµ¬ì¶• ê±´ë„ˆëœë‹ˆë‹¤.")
        return "ë¬¸ì„œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë²¡í„°DB êµ¬ì¶• ê±´ë„ˆëœë‹ˆë‹¤."

    try:
        splitter = make_splitter()
        chunks = splitter.split_documents(docs)
        if not chunks:
             print("í…ìŠ¤íŠ¸ ë¶„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë²¡í„°DB êµ¬ì¶• ê±´ë„ˆëœë‹ˆë‹¤.")
             return "í…ìŠ¤íŠ¸ ë¶„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë²¡í„°DB êµ¬ì¶• ê±´ë„ˆëœë‹ˆë‹¤."

        print(f"í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± ì™„ë£Œ: ì´ {len(chunks)}ê°œ")

        print("ë²¡í„° ì„ë² ë”© ìƒì„± ë° ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘...")
        vector_store = build_faiss(chunks) # build_faiss will check for API key internally
        print("ë²¡í„°DB ìƒì„±/ê°±ì‹  ì™„ë£Œ.")
        return f"ë²¡í„°DB ìƒì„±/ê°±ì‹  ì™„ë£Œ: ì´ {len(chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬."
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"ë²¡í„°DB êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"ë²¡í„°DB êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def build_vectorstore_with_progress(progress=gr.Progress()):
    """Orchestrates the autobuild process with progress reporting."""
    global OPENAI_API_KEY

    if not OPENAI_API_KEY:
         print("API í‚¤ ì—†ìŒ: ë°ì´í„° ì¤€ë¹„ ê±´ë„ˆëœë‹ˆë‹¤.")
         # Modified message to guide user on Hugging Face Secrets
         return "ğŸ”‘ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ë°ì´í„° ì¤€ë¹„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤. Hugging Face Secretsì— 'OPENAI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."


    steps = [
        "ë°ì´í„° í¬ë¡¤ë§ ì¤‘...",
        "ë¬¸ì„œ ë³€í™˜ ë° ë¶„í•  ì¤‘...",
        "ë²¡í„° ì„ë² ë”© ìƒì„± ë° ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘...",
        "ë§ˆë¬´ë¦¬ ì¤‘..."
    ]

    progress(0, desc=steps[0])
    crawl_result_msg = crawl_data_for_autobuild()
    time.sleep(0.5)

    progress(1/len(steps), desc=steps[1])
    time.sleep(0.5)

    progress(2/len(steps), desc=steps[2])
    build_result_msg = build_vectorstore_for_autobuild() # build_vectorstore_for_autobuild handles API key check
    time.sleep(0.5)

    progress(3/len(steps), desc=steps[3])
    time.sleep(0.5)

    final_message = f"âœ… ì¤€ë¹„ ì™„ë£Œ! {crawl_result_msg}, {build_result_msg}"
    print(final_message)
    return final_message


def rag_chatbot_for_autobuild(user_message: str, chat_history: list, user_profile: str) -> str:
    """Handles chatbot interaction with RAG and memory for the autobuild process (Retrieval separated)."""
    global vector_store, OPENAI_API_KEY, session_store

    if not OPENAI_API_KEY:
        print("API í‚¤ ì—†ìŒ: ì±—ë´‡ ì‘ë‹µ ë¶ˆê°€")
        # Modified message to guide user on Hugging Face Secrets
        return "ğŸ”‘ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Hugging Face Secretsì— 'OPENAI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."


    if vector_store is None:
        print("ë²¡í„°DB ì¤€ë¹„ ì•ˆë¨: ì±—ë´‡ ì‘ë‹µ ë¶ˆê°€")
        return "â³ ë°ì´í„° ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    session_id = "gradio_session_auto"
    history = get_history(session_id)

    try:
        retriever = vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 6, "fetch_k": 10, "lambda_mult": 0.5}
        )

        retrieved_docs = retriever.get_relevant_documents(str(user_message))

        qa_chain = build_qa_chain() # build_qa_chain will check for API key internally

        conversational_qa_chain = RunnableWithMessageHistory(
             qa_chain,
             get_history,
             input_messages_key="question",
             history_messages_key="history",
        )

        qa_input = {
             "question": str(user_message),
             "context": retrieved_docs,
             "profile": user_profile
        }

        answer = conversational_qa_chain.invoke(
             qa_input,
             config={"configurable": {"session_id": session_id}},
        )

        return answer

    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"ì±—ë´‡ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # Include API key check in error message if it's the likely cause
        if "AuthenticationError" in str(e) or "api_key" in str(e).lower():
             return "ğŸ”‘ OpenAI API í‚¤ ì¸ì¦ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. Hugging Face Secretsì— ì„¤ì •ëœ í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        else:
             return f"ì±—bot ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


# =============================================================================
# 5. Gradio UI ì •ì˜
# =============================================================================

def gradio_ui():
    with gr.Blocks() as demo:
        gr.Markdown("## ğŸï¸ ê´‘ì£¼ê´€ê´‘ ì¹œêµ¬")

        user_profile_state = gr.State("ì—¬í–‰ìŠ¤íƒ€ì¼:ììœ ì—¬í–‰, ë™í–‰:ì¹œêµ¬, ì˜ˆì‚°:ë³´í†µ")

        # ---- (A) ë¡œë”© í™”ë©´ ----
        with gr.Row(visible=True) as loading_screen:
            with gr.Column():
                gr.Markdown(
                    "### â³ ì‚°ì±…ì„ ìœ„í•´ ì¤€ë¹„ìš´ë™ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”! ğŸ°ğŸŒ¸"
                )
                gr.Image(
                    value="ê³ ì–‘ì´3.gif",
                    visible=True,
                    elem_id="loading_gif",
                )
                # Modified initial message to guide user on Hugging Face Secrets
                loading_status = gr.Textbox(label="ì§„í–‰ ìƒíƒœ", interactive=False, show_label=False, value="ì‹œì‘ ì¤€ë¹„ ì¤‘... Hugging Face Secretsì— 'OPENAI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")


        # ---- (B) ì±—ë´‡ í™”ë©´ (ì´ˆê¸°ì—ëŠ” ìˆ¨ê¹€) ----
        with gr.Column(visible=False) as chatbot_screen:
            gr.Markdown("### ğŸ¤– ê´‘ì£¼ ê´€ê´‘ ê°€ì´ë“œ ì±—ë´‡")
            chatbot = gr.Chatbot(label="ì±—ë´‡", type='messages')
            msg = gr.Textbox(placeholder="ê´‘ì£¼ ê´€ê´‘ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”!", show_label=False)
            clear = gr.Button("ëŒ€í™” ì´ˆê¸°í™”")

            # --- User Profile Input ---
            with gr.Accordion("ì‚¬ìš©ì í”„ë¡œí•„ ì„¤ì •", open=False):
                profile_style = gr.Dropdown(label="ì—¬í–‰ ìŠ¤íƒ€ì¼", choices=["ììœ ì—¬í–‰", "ë‹¨ì²´ì—¬í–‰", "ë°˜ë ¤ë™ë¬¼ ë™ë°˜", "ëšœë²…ì´ì—¬í–‰"], value="ììœ ì—¬í–‰")
                profile_companion = gr.Dropdown(label="ë™í–‰", choices=["í˜¼ì", "ì¹œêµ¬", "ê°€ì¡±", "ì—°ì¸"], value="ì¹œêµ¬")
                profile_budget = gr.Dropdown(label="ì˜ˆì‚°", choices=["ìƒê´€ì—†ìŒ", "ì €ë ´", "ë³´í†µ", "ì—¬ìœ "], value="ë³´í†µ")
                profile_update_btn = gr.Button("í”„ë¡œí•„ ì—…ë°ì´íŠ¸")

                def update_profile_state(style, companion, budget):
                    profile_str = f"ì—¬í–‰ìŠ¤íƒ€ì¼:{style}, ë™í–‰:{companion}, ì˜ˆì‚°:{budget}"
                    print(f"ì‚¬ìš©ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸: {profile_str}")
                    return profile_str

                profile_update_btn.click(
                    update_profile_state,
                    inputs=[profile_style, profile_companion, profile_budget],
                    outputs=user_profile_state
                )

            def user_chat(user_message, history, current_profile):
                if not user_message:
                    return history, ""

                response = rag_chatbot_for_autobuild(user_message, history, current_profile)

                # Ensure history format is compatible with type='messages'
                # The response is a string, need to format it as a message dictionary
                new_history = history + [{"role": "user", "content": user_message}, {"role": "assistant", "content": response}]

                return new_history, ""

            msg.submit(
                user_chat,
                inputs=[msg, chatbot, user_profile_state],
                outputs=[chatbot, msg]
            )
            clear.click(lambda: [], inputs=None, outputs=chatbot)

            # --- OpenAI API Key Input (Removed as key is handled by Secrets) ---
            # gr.Markdown("### ğŸ”‘ OpenAI API Key ì„¤ì •")
            # openai_key_input = gr.Textbox(...)
            # def set_openai_key(key): ...
            # openai_key_input.change(...)


        # ---- (C) ì‹¤í–‰ ì‹œ ìë™ DB êµ¬ì¶• íŠ¸ë¦¬ê±° ----
        def init_build(progress=gr.Progress()):
            print("init_build function started by demo.load")
            # The build_vectorstore_with_progress function now includes API key check
            status_message = build_vectorstore_with_progress(progress)
            print(f"init_build finished with status: {status_message}")
            # Only switch to chatbot screen if build was successful (vector_store is not None)
            if vector_store is not None:
                return gr.update(visible=False), gr.update(visible=True), status_message
            else:
                # Stay on loading screen if build failed (e.g., missing API key)
                return gr.update(visible=True), gr.update(visible=False), status_message


        demo.load(
            init_build,
            inputs=None,
            outputs=[loading_screen, chatbot_screen, loading_status]
        )

    return demo

# =============================================================================
# 6. ì‹¤í–‰
# =============================================================================
if __name__ == "__main__":
    print("ì•± ì‹¤í–‰ ì‹œì‘...")

    try:
        import gradio
        gradio.close_all()
        print("ê¸°ì¡´ Gradio ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ ì‹œë„.")
    except Exception as e:
        print(f"Gradio ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œ ê°€ëŠ¥): {e}")

    # OPENAI_API_KEY is now loaded globally at the top from os.getenv

    if not OPENAI_API_KEY:
        print("ê²½ê³ : OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•±ì´ ì‹œì‘ë˜ì§€ë§Œ ë²¡í„°DB êµ¬ì¶• ë° ì±—ë´‡ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤. Hugging Face Secretsì— 'OPENAI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    else:
        print("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ ê°ì§€ë¨.")

    demo = gradio_ui()
    print("Gradio UI ì‹¤í–‰ ì¤‘...")
    # Removed share=True for default Hugging Face Spaces deployment
    demo.launch(debug=True)
