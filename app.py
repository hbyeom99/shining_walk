# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª…ì‹œì  ì„¤ì¹˜ (ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´ ì´ ì…€ì„ ê±´ë„ˆë›°ê±°ë‚˜ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”)
!pip install -q langchain-openai
!pip install -q langchain-community
!pip install -q pypdf beautifulsoup4 requests tqdm gradio # Include pypdf here

import os
import gradio as gr
import time
import re
import json
import hashlib
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple
from collections import deque

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

# --- Secure key load ---
# OPENAI_API_KEY is loaded from Hugging Face Secrets or environment variables
# In Colab for testing, attempt to load from secrets or define manually
try:
    from google.colab import userdata
    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')
    if not OPENAI_API_KEY:
         print("WARNING: 'OPENAI_API_KEY' not found in Colab secrets. Please add it or define it manually.")
except Exception as e:
    print(f"WARNING: Could not load OPENAI_API_KEY from Colab secrets: {e}")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") # Fallback to env var


# --- LangChain imports ---
# Make sure these are installed:
# !pip install -q langchain-community langchain-openai langchain-text-splitters faiss-cpu
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# Import FAISS module explicitly for loading
from langchain_community.vectorstores.faiss import FAISS as FAISS_Vectorstore

# Import pypdf explicitly as it's used by PyPDFLoader internally
import pypdf


# =============================================================================
# 2. ë°ì´í„° êµ¬ì¡° ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ë°ì´í„° ì¤€ë¹„)
# =============================================================================

# Crawling and PDF loading functions are removed or commented out
# BASE_URL = "https://tour.gwangju.go.kr" (...)
# def crawl_home_and_details(...)
# def load_data_from_pdf(...)

def make_splitter() -> RecursiveCharacterTextSplitter:
    # Splitter is still needed if you rebuild from source or process new docs,
    # but not strictly for loading a saved index. Keep for consistency maybe.
    return RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100, separators=["\n\n", "\n", ". ", ".", " "])


# =============================================================================
# 3. ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ë° RAG ì²´ì¸ ì„¤ì •
# =============================================================================

# Global variables to maintain state
vector_store: Optional[FAISS] = None # Will be loaded from file
session_store: Dict[str, BaseChatMessageHistory] = {}

# build_faiss function is not needed if loading from saved index
# def build_faiss(docs: List[Document]) -> FAISS: (...)

def build_qa_chain(model_name: str = "gpt-4o-mini"):
    global OPENAI_API_KEY
    if not OPENAI_API_KEY:
         raise ValueError("OpenAI API key not set. Cannot build QA chain.")

    # Using raw string and explicit newlines for system prompt
    system_prompt = (
'''ë‹¹ì‹ ì€ ì´ì œ ê´‘ì£¼ ê´€ê´‘ì— ëŒ€í•œ ê¹Šì€ ì§€ì‹ê³¼ ì—´ì •ì„ ê°€ì§„ ìµœê³ ì˜ ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë„ˆëŠ” ê´‘ì£¼ê´‘ì—­ì‹œ ê³µì‹ ê´€ê´‘ í¬í„¸ì˜ ì½˜í…ì¸ ì™€ ì¶”ê°€ PDFë¥¼ ìš”ì•½/ê²€ìƒ‰í•´ì£¼ëŠ”
ê´€ê´‘ ë„ìš°ë¯¸ ì—­í• ì„ ìˆ˜í–‰í•˜ë©°, ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³ ,
ë§ˆì¹˜ ì¹œí•œ ì—¬í–‰ ê°€ì´ë“œì²˜ëŸ¼ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.
ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ë‹µë³€í•´ì¤˜, ì´ì „ì— ì–¸ê¸‰í•œ ë‚´ìš©ì€ ë°˜ë³µí•˜ì§€ ì•Šë„ë¡ ë…¸ë ¥í•´ì¤˜
ë¹„ìŠ·í•œ ë‚´ìš©ì„ ë¬¼ì–´ë³¸ë‹¤ë©´ ê¸°ì¡´ì— ë‹µí–ˆë˜ ë‚´ìš©ì€ ì¤‘ë³µë˜ì—ˆë‹¤ëŠ” ëœ»ì´ë‹ˆ ì œì™¸í•˜ê³  ë‹¤ë¥¸ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ì„œ ì•Œë ¤ì¤˜
ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ë¡œ ì§€ì–´ë‚´ì„œ ì•Œë ¤ì£¼ë©´ ì•ˆë˜ë©° ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ê´€ë ¨ëœ ì£¼ì†Œë¥¼ ë§í¬í•´ì„œ ëŒ€ì‹  ëŒ€ë‹µí•´ì¤˜

ë”±ë”±í•œ ì •ë³´ ë‚˜ì—´ë³´ë‹¤ëŠ” ë§¤ë ¥ì ì¸ ì—¬í–‰ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ë“¯ ë‹µë³€í•´ì£¼ì„¸ìš”.
ê´‘ì£¼ì˜ ìˆ¨ê²¨ì§„ ëª…ì†Œ, ë§›ì§‘, í–‰ì‚¬, êµí†µ ì •ë³´ ë“± ì—¬í–‰ê°ì—ê²Œ ê¼­ í•„ìš”í•œ
ì‹¤ì§ˆì ì¸ ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. ì‚¬ìš©ìì˜ ì—¬í–‰ ìŠ¤íƒ€ì¼ê³¼ ì˜ˆì‚°ì„
ê³ ë ¤í•˜ì—¬ ë§ì¶¤í˜• ì¶”ì²œì„ ë§ë¶™ì—¬ì£¼ë©´ ë”ìš± ì¢‹ìŠµë‹ˆë‹¤.

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ìµœì¢…ì— ì •ë³´ì˜ ì¶œì²˜ ë§í¬ë¥¼ bulletìœ¼ë¡œ ëª…í™•í•˜ê²Œ í¬í•¨í•˜ì—¬
ì‹ ë¢°ë„ë¥¼ ë†’ì—¬ì£¼ì„¸ìš”.'''
    )

    template = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "ì‚¬ìš©ì í”„ë¡œí•„: {profile}\n\nì§ˆë¬¸: {question}\n\nì°¸ê³  ì»¨í…ìŠ¤íŠ¸: {context}\n\nìš”ì²­: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ë‹µí•˜ê³ , ë¶€ì¡±í•˜ë©´ ì¼ë°˜ ìƒì‹ì€ í”¼í•˜ê³  'ê³µì‹ í¬í„¸ì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤'ë¼ê³  ë§í•´. í•œêµ­ì–´ë¡œ ë‹µí•˜ê¸°. ìµœì¢…ì— ì¶œì²˜ë¥¼ bulletë¡œ ì •ë¦¬."),
    ])

    llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=model_name, temperature=0.2)

    def format_docs(docs: List[Document]) -> str:
        chunks = []
        added_sources = set()
        for i, d in enumerate(docs):
            t = d.metadata.get("title", f"ë¬¸ì„œ {i+1}")
            s = d.page_content[:500]
            src = d.metadata.get("source", "ì¶œì²˜ ë¶ˆëª…") # 'source' should now contain original URL from crawled data
            chunks.append(f"- **{t}** ([ì¶œì²˜]({src}))\n{s}...") # Use \n for literal newline in f-string
            added_sources.add(src)

        sources_list = "\n\n**ì°¸ê³  ìë£Œ:**\n" + "\n".join([f"- {s}" for s in added_sources]) if added_sources else ""

        return "\n\n".join(chunks) + sources_list


    qa_chain = (
         RunnableParallel({
             "context": lambda x: format_docs(x["context"]),
             "question": RunnablePassthrough(),
             "profile": RunnablePassthrough()
         })
        | template
        | llm
        | StrOutputParser()
    )

    return qa_chain

# Session history store
store: Dict[str, BaseChatMessageHistory] = {}
def get_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


# =============================================================================
# 4. Gradio ì•± ë¡œì§ í•¨ìˆ˜ (ìë™ ë¹Œë“œ/ë¡œë”© ë° ì±—ë´‡ ì²˜ë¦¬)
# =============================================================================

# build_vectorstore_for_autobuild is replaced by load_vectorstore
# def build_vectorstore_for_autobuild() -> str: (...)

def load_vectorstore_from_saved() -> str:
    """Loads the vectorstore from the saved directory."""
    global vector_store, OPENAI_API_KEY

    print("ë²¡í„°DB ë¡œë“œ í•¨ìˆ˜ ì‹œì‘...")

    if not OPENAI_API_KEY:
         print("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë²¡í„°DB ë¡œë“œ ê±´ë„ˆëœë‹ˆë‹¤.")
         return "ğŸ”‘ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ë²¡í„°DB ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤. Hugging Face Secretsì— 'OPENAI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."

    save_dir = "faiss_index"
    if not os.path.exists(save_dir) or not os.path.exists(os.path.join(save_dir, "index.faiss")) or not os.path.exists(os.path.join(save_dir, "index.pkl")):
        print(f"ê²½ê³ : ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ ë””ë ‰í† ë¦¬ '{save_dir}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ íŒŒì¼ì´ ì™„ì „í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë²¡í„°DB ë¡œë“œ ê±´ë„ˆëœë‹ˆë‹¤.")
        return f"âš ï¸ ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ ë””ë ‰í† ë¦¬ '{save_dir}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë²¡í„° ìŠ¤í† ì–´ë¥¼ êµ¬ì¶•í•˜ê³  ì €ì¥í•´ì£¼ì„¸ìš”."

    try:
        print(f"'{save_dir}'ì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
        # Need embeddings object to load FAISS
        embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
        vector_store = FAISS_Vectorstore.load_local(save_dir, embeddings, allow_dangerous_deserialization=True)
        print("ë²¡í„°DB ë¡œë“œ ì™„ë£Œ.")
        return f"âœ… ë²¡í„°DB ë¡œë“œ ì™„ë£Œ: '{save_dir}'ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"ë²¡í„°DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # Include API key check in error message if it's the likely cause
        if "AuthenticationError" in str(e) or "api_key" in str(e).lower():
             return "ğŸ”‘ OpenAI API í‚¤ ì¸ì¦ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. Hugging Face Secretsì— ì„¤ì •ëœ í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        else:
             return f"ë²¡í„°DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


def build_or_load_vectorstore_with_progress(progress=gr.Progress()):
    """Orchestrates the vectorstore loading process with progress reporting."""
    global OPENAI_API_KEY

    if not OPENAI_API_KEY:
         print("API í‚¤ ì—†ìŒ: ë°ì´í„° ì¤€ë¹„ ê±´ë„ˆëœë‹ˆë‹¤.")
         return "ğŸ”‘ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ë°ì´í„° ì¤€ë¹„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤. Hugging Face Secretsì— 'OPENAI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."


    steps = [
        "ì €ì¥ëœ ë²¡í„°DB ë¡œë“œ ì¤‘...", # Updated step description
        "ë§ˆë¬´ë¦¬ ì¤‘..."
    ]

    progress(0, desc=steps[0])
    status_message = load_vectorstore_from_saved() # Call the loading function
    time.sleep(0.5)

    progress(1/len(steps), desc=steps[1])
    time.sleep(0.5)

    final_message = f"âœ… ì¤€ë¹„ ì™„ë£Œ! {status_message}" # Updated final message
    print(final_message)
    return final_message


def rag_chatbot_for_autobuild(user_message: str, chat_history: list, user_profile: str) -> str:
    """Handles chatbot interaction with RAG and memory."""
    global vector_store, OPENAI_API_KEY, session_store

    if not OPENAI_API_KEY:
        print("API í‚¤ ì—†ìŒ: ì±—ë´‡ ì‘ë‹µ ë¶ˆê°€")
        return "ğŸ”‘ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Hugging Face Secretsì— 'OPENAI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."


    if vector_store is None:
        print("ë²¡í„°DB ì¤€ë¹„ ì•ˆë¨: ì±—ë´‡ ì‘ë‹µ ë¶ˆê°€")
        return "â³ ë°ì´í„° ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    session_id = "gradio_session_auto"
    history = get_history(session_id)

    try:
        retriever = vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 6, "fetch_k": 10, "lambda_mult": 0.5}
        )

        retrieved_docs = retriever.get_relevant_documents(str(user_message))

        qa_chain = build_qa_chain() # build_qa_chain will check for API key internally

        conversational_qa_chain = RunnableWithMessageHistory(
             qa_chain,
             get_history,
             input_messages_key="question",
             history_messages_key="history",
        )

        qa_input = {
             "question": str(user_message),
             "context": retrieved_docs,
             "profile": user_profile
        }

        answer = conversational_qa_chain.invoke(
             qa_input,
             config={"configurable": {"session_id": session_id}},
        )

        return answer

    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"ì±—ë´‡ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # Include API key check in error message if it's the likely cause
        if "AuthenticationError" in str(e) or "api_key" in str(e).lower():
             return "ğŸ”‘ OpenAI API í‚¤ ì¸ì¦ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. Hugging Face Secretsì— ì„¤ì •ëœ í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        else:
             return f"ì±—bot ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


# =============================================================================
# 5. Gradio UI ì •ì˜
# =============================================================================

def gradio_ui():
    with gr.Blocks() as demo:
        gr.Markdown("## ğŸï¸ ê´‘ì£¼ê´€ê´‘ ì¹œêµ¬")

        user_profile_state = gr.State("ì—¬í–‰ìŠ¤íƒ€ì¼:ììœ ì—¬í–‰, ë™í–‰:ì¹œêµ¬, ì˜ˆì‚°:ë³´í†µ")

        # ---- (A) ë¡œë”© í™”ë©´ ----
        with gr.Row(visible=True) as loading_screen:
            with gr.Column():
                gr.Markdown(
                    "### â³ ì‚°ì±…ì„ ìœ„í•´ ì¤€ë¹„ìš´ë™ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”! ğŸ°ğŸŒ¸"
                )
                # Using a public URL for the image instead of a local file
                gr.Image(
                    value="https://cdn.pixabay.com/animation/2025/07/25/00/29/00-29-46-321_512.gif", # Updated image URL
                    visible=True,
                    elem_id="loading_gif",
                )
                # Modified initial message to indicate vectorstore loading
                loading_status = gr.Textbox(label="ì§„í–‰ ìƒíƒœ", interactive=False, show_label=False, value="ì‹œì‘ ì¤€ë¹„ ì¤‘... ì €ì¥ëœ ë²¡í„°DB ë¡œë“œ ì¤‘...")


        # ---- (B) ì±—ë´‡ í™”ë©´ (ì´ˆê¸°ì—ëŠ” ìˆ¨ê¹€) ----
        with gr.Column(visible=False) as chatbot_screen:
            gr.Markdown("### ğŸ¤– ê´‘ì£¼ ê´€ê´‘ ê°€ì´ë“œ ì±—ë´‡")
            chatbot = gr.Chatbot(label="ì±—ë´‡", type='messages')
            msg = gr.Textbox(placeholder="ê´‘ì£¼ ê´€ê´‘ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”!", show_label=False)
            clear = gr.Button("ëŒ€í™” ì´ˆê¸°í™”")

            # --- User Profile Input ---
            with gr.Accordion("ì‚¬ìš©ì í”„ë¡œí•„ ì„¤ì •", open=False):
                profile_style = gr.Dropdown(label="ì—¬í–‰ ìŠ¤íƒ€ì¼", choices=["ììœ ì—¬í–‰", "ë‹¨ì²´ì—¬í–‰", "ë°˜ë ¤ë™ë¬¼ ë™ë°˜", "ëšœë²…ì´ì—¬í–‰"], value="ììœ ì—¬í–‰")
                profile_companion = gr.Dropdown(label="ë™í–‰", choices=["í˜¼ì", "ì¹œêµ¬", "ê°€ì¡±", "ì—°ì¸"], value="ì¹œêµ¬")
                profile_budget = gr.Dropdown(label="ì˜ˆì‚°", choices=["ìƒê´€ì—†ìŒ", "ì €ë ´", "ë³´í†µ", "ì—¬ìœ "], value="ë³´í†µ")
                profile_update_btn = gr.Button("í”„ë¡œí•„ ì—…ë°ì´íŠ¸")

                def update_profile_state(style, companion, budget):
                    profile_str = f"ì—¬í–‰ìŠ¤íƒ€ì¼:{style}, ë™í–‰:{companion}, ì˜ˆì‚°:{budget}"
                    print(f"ì‚¬ìš©ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸: {profile_str}")
                    return profile_str

                profile_update_btn.click(
                    update_profile_state,
                    inputs=[profile_style, profile_companion, profile_budget],
                    outputs=[user_profile_state]
                )

            def user_chat(user_message: str, chat_history: list, current_profile: str) -> Tuple[list, str]:
                if not user_message:
                    return chat_history, ""

                response = rag_chatbot_for_autobuild(user_message, chat_history, current_profile)

                # Ensure history format is compatible with type='messages'
                # The response is a string, need to format it as a message dictionary
                new_history = chat_history + [{"role": "user", "content": user_message}, {"role": "assistant", "content": response}]

                return new_history, ""

            msg.submit(
                user_chat,
                inputs=[msg, chatbot, user_profile_state],
                outputs=[chatbot, msg]
            )
            clear.click(lambda: [], inputs=None, outputs=chatbot)


        # ---- (C) ì‹¤í–‰ ì‹œ ìë™ DB ë¡œë“œ íŠ¸ë¦¬ê±° ----
        def init_load(progress=gr.Progress()):
            print("init_load function started by demo.load")
            # The load_vectorstore_from_saved function now handles API key and file checks
            status_message = load_vectorstore_from_saved()
            print(f"init_load finished with status: {status_message}")
            # Only switch to chatbot screen if loading was successful (vector_store is not None)
            if vector_store is not None:
                return gr.update(visible=False), gr.update(visible=True), status_message
            else:
                # Stay on loading screen if loading failed (e.g., missing API key or index)
                return gr.update(visible=True), gr.update(visible=False), status_message


        # Use demo.load() to trigger init_load when the Gradio app starts
        demo.load(
            init_load,
            inputs=None,
            outputs=[loading_screen, chatbot_screen, loading_status]
        )

    return demo

# =============================================================================
# 6. ì‹¤í–‰
# =============================================================================
# This section is included for direct execution within a single Colab cell
if __name__ == "__main__":
    print("ì•± ì‹¤í–‰ ì‹œì‘...")

    # Ensure necessary libraries are installed for this single cell execution
    try:
        import gradio
        import langchain_community # Check if common libraries are available
        import langchain_openai
        import faiss # faiss-cpu is the package name, but faiss is the import
        import bs4 # Check for beautifulsoup4 import name
        import requests
        import tqdm
        import pypdf # Add pypdf to the check
    except ImportError:
        print("Installing necessary libraries...")
        # Include pypdf and beautifulsoup4 in the installation list
        !pip install -q langchain-community langchain-openai langchain-text-splitters faiss-cpu pypdf beautifulsoup4 requests tqdm gradio

    # Define OPENAI_API_KEY here for Colab testing if not already defined
    # Replace with your actual key or load from secrets if preferred for testing
    # Make sure the 'faiss_index' directory exists and contains index.faiss and index.pkl
    try:
        # Attempt to load from Colab secrets first
        from google.colab import userdata
        OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')
        if not OPENAI_API_KEY:
             print("WARNING: 'OPENAI_API_KEY' not found in Colab secrets. Please add it or define it manually.")
    except Exception as e:
        print(f"WARNING: Could not load OPENAI_API_KEY from Colab secrets: {e}")
        # Fallback to environment variable if running outside Colab secrets context
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


    # Check for API key again before launching demo if needed for loading vectorstore
    if not OPENAI_API_KEY:
         print("\nì£¼ì˜: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Gradio ì•±ì€ ì‹œì‘ë˜ì§€ë§Œ, ë²¡í„°DB ë¡œë“œ ë° ì±—ë´‡ ê¸°ëŠ¥ì€ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Hugging Face Secretsì— 'OPENAI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
         # You might want to skip launching the demo here if API key is essential
         # Or let the load_vectorstore_from_saved function handle the message

    demo = gradio_ui()
    print("Gradio UI ì‹¤í–‰ ì¤‘...")
    # Removed share=True for default Hugging Face Spaces deployment
    # You might need share=True if you want a public link outside Colab
    demo.launch(debug=True)
]

app_py_code = "\n".join(app_py_code_lines)

with open("app.py", "w", encoding="utf-8") as f:
    f.write(app_py_code)